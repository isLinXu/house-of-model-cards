OneFormerModel(
  (pixel_level_module): OneFormerPixelLevelModule(
    (encoder): SwinBackbone(
      (embeddings): SwinEmbeddings(
        (patch_embeddings): SwinPatchEmbeddings(
          (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
        )
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (encoder): SwinEncoder(
        (layers): ModuleList(
          (0): SwinStage(
            (blocks): ModuleList(
              (0-1): 2 x SwinLayer(
                (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attention): SwinAttention(
                  (self): SwinSelfAttention(
                    (query): Linear(in_features=96, out_features=96, bias=True)
                    (key): Linear(in_features=96, out_features=96, bias=True)
                    (value): Linear(in_features=96, out_features=96, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): SwinSelfOutput(
                    (dense): Linear(in_features=96, out_features=96, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): SwinDropPath(p=0.3)
                (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (intermediate): SwinIntermediate(
                  (dense): Linear(in_features=96, out_features=384, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): SwinOutput(
                  (dense): Linear(in_features=384, out_features=96, bias=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (downsample): SwinPatchMerging(
              (reduction): Linear(in_features=384, out_features=192, bias=False)
              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): SwinStage(
            (blocks): ModuleList(
              (0-1): 2 x SwinLayer(
                (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (attention): SwinAttention(
                  (self): SwinSelfAttention(
                    (query): Linear(in_features=192, out_features=192, bias=True)
                    (key): Linear(in_features=192, out_features=192, bias=True)
                    (value): Linear(in_features=192, out_features=192, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): SwinSelfOutput(
                    (dense): Linear(in_features=192, out_features=192, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): SwinDropPath(p=0.3)
                (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (intermediate): SwinIntermediate(
                  (dense): Linear(in_features=192, out_features=768, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): SwinOutput(
                  (dense): Linear(in_features=768, out_features=192, bias=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (downsample): SwinPatchMerging(
              (reduction): Linear(in_features=768, out_features=384, bias=False)
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): SwinStage(
            (blocks): ModuleList(
              (0-5): 6 x SwinLayer(
                (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attention): SwinAttention(
                  (self): SwinSelfAttention(
                    (query): Linear(in_features=384, out_features=384, bias=True)
                    (key): Linear(in_features=384, out_features=384, bias=True)
                    (value): Linear(in_features=384, out_features=384, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): SwinSelfOutput(
                    (dense): Linear(in_features=384, out_features=384, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): SwinDropPath(p=0.3)
                (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (intermediate): SwinIntermediate(
                  (dense): Linear(in_features=384, out_features=1536, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): SwinOutput(
                  (dense): Linear(in_features=1536, out_features=384, bias=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (downsample): SwinPatchMerging(
              (reduction): Linear(in_features=1536, out_features=768, bias=False)
              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): SwinStage(
            (blocks): ModuleList(
              (0-1): 2 x SwinLayer(
                (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (attention): SwinAttention(
                  (self): SwinSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): SwinSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): SwinDropPath(p=0.3)
                (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (intermediate): SwinIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): SwinOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (dropout): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
        )
      )
      (hidden_states_norms): ModuleDict(
        (stage1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (stage2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (stage3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (stage4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (decoder): OneFormerPixelDecoder(
      (position_embedding): OneFormerSinePositionEmbedding()
      (input_projections): ModuleList(
        (0): Sequential(
          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (encoder): OneFormerPixelDecoderEncoderOnly(
        (layers): ModuleList(
          (0-5): 6 x OneFormerPixelDecoderEncoderLayer(
            (self_attn): OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(
              (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
              (attention_weights): Linear(in_features=256, out_features=96, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Sequential(
        (0): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        (2): ReLU()
      )
    )
  )
  (transformer_module): OneFormerTransformerModule(
    (position_embedder): OneFormerSinePositionEmbedding()
    (queries_embedder): Embedding(150, 256)
    (decoder): OneFormerTransformerDecoder(
      (query_transformer): OneFormerTransformerDecoderQueryTransformer(
        (decoder): OneFormerTransformerDecoderQueryTransformerDecoder(
          (layers): ModuleList(
            (0-1): 2 x OneFormerTransformerDecoderQueryTransformerDecoderLayer(
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (multihead_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
              )
              (linear1): Linear(in_features=256, out_features=2048, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (linear2): Linear(in_features=2048, out_features=256, bias=True)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (dropout1): Dropout(p=0.1, inplace=False)
              (dropout2): Dropout(p=0.1, inplace=False)
              (dropout3): Dropout(p=0.1, inplace=False)
              (activation): ReLU()
            )
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (layers): ModuleList(
        (0-8): 9 x OneFormerTransformerDecoderLayer(
          (cross_attn): OneFormerTransformerDecoderCrossAttentionLayer(
            (multihead_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (activation): ReLU()
          )
          (self_attn): OneFormerTransformerDecoderSelfAttentionLayer(
            (self_attn): OneFormerAttention(
              (k_proj): Linear(in_features=256, out_features=256, bias=True)
              (v_proj): Linear(in_features=256, out_features=256, bias=True)
              (q_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (activation): ReLU()
          )
          (ffn): OneFormerTransformerDecoderFFNLayer(
            (linear1): Linear(in_features=256, out_features=2048, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=2048, out_features=256, bias=True)
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (activation): ReLU()
          )
        )
      )
      (query_input_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): OneFormerMLPPredictionHead(
        (layers): Sequential(
          (0): PredictionBlock(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): ReLU()
          )
          (1): PredictionBlock(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): ReLU()
          )
          (2): PredictionBlock(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Identity()
          )
        )
      )
    )
    (level_embed): Embedding(3, 256)
  )
  (task_encoder): OneFormerTaskModel(
    (task_mlp): OneFormerMLPPredictionHead(
      (layers): Sequential(
        (0): PredictionBlock(
          (0): Linear(in_features=77, out_features=256, bias=True)
          (1): ReLU()
        )
        (1): PredictionBlock(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Identity()
        )
      )
    )
  )
)
