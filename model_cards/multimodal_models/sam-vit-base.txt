SamModel(
  (shared_image_embedding): SamPositionalEmbedding()
  (vision_encoder): SamVisionEncoder(
    (patch_embed): SamPatchEmbeddings(
      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (layers): ModuleList(
      (0-11): 12 x SamVisionLayer(
        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): SamVisionAttention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): SamMLPBlock(
          (lin1): Linear(in_features=768, out_features=3072, bias=True)
          (lin2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELUActivation()
        )
      )
    )
    (neck): SamVisionNeck(
      (conv1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (layer_norm1): SamLayerNorm()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (layer_norm2): SamLayerNorm()
    )
  )
  (prompt_encoder): SamPromptEncoder(
    (shared_embedding): SamPositionalEmbedding()
    (mask_embed): SamMaskEmbedding(
      (activation): GELUActivation()
      (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
      (layer_norm1): SamLayerNorm()
      (layer_norm2): SamLayerNorm()
    )
    (no_mask_embed): Embedding(1, 256)
    (point_embed): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
  )
  (mask_decoder): SamMaskDecoder(
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (transformer): SamTwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x SamTwoWayAttentionBlock(
          (self_attn): SamAttention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (cross_attn_token_to_image): SamAttention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (mlp): SamMLPBlock(
            (lin1): Linear(in_features=256, out_features=2048, bias=True)
            (lin2): Linear(in_features=2048, out_features=256, bias=True)
            (act): ReLU()
          )
          (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (cross_attn_image_to_token): SamAttention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): SamAttention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
    (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
    (upscale_layer_norm): SamLayerNorm()
    (activation): GELU(approximate='none')
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x SamFeedForward(
        (activation): ReLU()
        (proj_in): Linear(in_features=256, out_features=256, bias=True)
        (proj_out): Linear(in_features=256, out_features=32, bias=True)
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
    (iou_prediction_head): SamFeedForward(
      (activation): ReLU()
      (proj_in): Linear(in_features=256, out_features=256, bias=True)
      (proj_out): Linear(in_features=256, out_features=4, bias=True)
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
      )
    )
  )
)
